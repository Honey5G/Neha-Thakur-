{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment Code: DA-AG-012**"
      ],
      "metadata": {
        "id": "rcK3P8u1S9To"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision Tree | Assignment**"
      ],
      "metadata": {
        "id": "8L1s-hoGS4jX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques.1 What is a Decision Tree, and how does it work in the context of\n",
        "classification?"
      ],
      "metadata": {
        "id": "3Bpod5iSS6Gj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:1"
      ],
      "metadata": {
        "id": "iH7gAutaTTC0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f662da3e"
      },
      "source": [
        "Ans: A Decision Tree is a flowchart-like structure used in machine learning for both classification and regression tasks. In the context of classification, it works by recursively partitioning the data based on the values of different features.\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "1.  **Starting Node (Root Node):** The process begins at the root node, which represents the entire dataset. The algorithm selects the \"best\" feature to split the data based on a criterion like Gini impurity or entropy, aiming to maximize the information gain.\n",
        "\n",
        "2.  **Splitting:** The dataset is divided into subsets based on the values of the chosen feature. Each subset becomes a child node.\n",
        "\n",
        "3.  **Decision Nodes:** The child nodes are further split based on other features, creating more branches. This process continues until a stopping condition is met.\n",
        "\n",
        "4.  **Leaf Nodes:** The final nodes in the tree are called leaf nodes. Each leaf node represents a class label (in the case of classification). All data points that reach a particular leaf node are assigned to that class.\n",
        "\n",
        "5.  **Making Predictions:** To classify a new data point, you traverse the tree from the root node down to a leaf node by following the branches based on the data point's feature values. The class label of the leaf node is the predicted class for that data point.\n",
        "\n",
        "In essence, a Decision Tree learns a set of rules from the data that can be used to classify new instances. It's intuitive to understand and visualize, making it a popular choice for various classification problems."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques.2 Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?"
      ],
      "metadata": {
        "id": "5K660hm9TcwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:2"
      ],
      "metadata": {
        "id": "_Mjpm5xSTlDP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5874c7f"
      },
      "source": [
        "Ans: Gini Impurity and Entropy are two common impurity measures used in Decision Trees to determine the \"best\" split at each node. They quantify the randomness or mixed-upness of the class labels within a subset of data. The goal is to find splits that minimize impurity in the resulting child nodes.\n",
        "\n",
        "Here's a breakdown of each:\n",
        "\n",
        "*   **Gini Impurity:**\n",
        "    *   Measures the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the distribution of labels in the subset.\n",
        "    *   A Gini impurity of 0 means all elements in the subset belong to the same class (perfectly pure).\n",
        "    *   A higher Gini impurity indicates a more mixed set of classes.\n",
        "    *   The formula for Gini Impurity is: $Gini = 1 - \\sum_{i=1}^{C} (p_i)^2$, where $p_i$ is the probability of an element belonging to class $i$, and $C$ is the number of classes.\n",
        "\n",
        "*   **Entropy:**\n",
        "    *   Measures the average amount of information needed to identify the class of an element in the subset.\n",
        "    *   An entropy of 0 means all elements in the subset belong to the same class (perfectly pure).\n",
        "    *   A higher entropy indicates a more mixed set of classes and more uncertainty.\n",
        "    *   The formula for Entropy is: $Entropy = - \\sum_{i=1}^{C} p_i \\log_2(p_i)$, where $p_i$ is the probability of an element belonging to class $i$, and $C$ is the number of classes.\n",
        "\n",
        "**Impact on Splits:**\n",
        "\n",
        "Decision Tree algorithms use these impurity measures to evaluate potential splits. At each node, the algorithm considers splitting the data based on different features and their values. For each potential split, it calculates the impurity of the resulting child nodes. The split that results in the largest **information gain** is chosen.\n",
        "\n",
        "Information gain is the reduction in impurity achieved by a split. It is calculated as:\n",
        "\n",
        "$Information Gain = Impurity(Parent Node) - \\sum_{j=1}^{k} \\frac{N_j}{N} Impurity(Child Node_j)$\n",
        "\n",
        "Where:\n",
        "*   $Impurity(Parent Node)$ is the impurity of the node before the split.\n",
        "*   $k$ is the number of child nodes created by the split.\n",
        "*   $N_j$ is the number of data points in child node $j$.\n",
        "*   $N$ is the total number of data points in the parent node.\n",
        "*   $Impurity(Child Node_j)$ is the impurity of child node $j$.\n",
        "\n",
        "The algorithm selects the split that maximizes information gain, as this split best separates the data into more homogeneous subsets with respect to the class labels. Both Gini Impurity and Entropy serve this purpose, and while they use different formulas, they generally lead to similar decision trees in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques.3 What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each?"
      ],
      "metadata": {
        "id": "rflpxDyVTzoR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:3"
      ],
      "metadata": {
        "id": "0zy6icIVT9YC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d365440"
      },
      "source": [
        "Ans: Pre-pruning and Post-pruning are techniques used to prevent overfitting in Decision Trees by reducing the complexity of the tree.\n",
        "\n",
        "Here's the difference:\n",
        "\n",
        "*   **Pre-pruning (Early Stopping):**\n",
        "    *   This technique stops the tree growth process early during the training phase.\n",
        "    *   It sets criteria to stop splitting a node before it becomes a perfect classifier for the training data. These criteria can include:\n",
        "        *   Maximum tree depth.\n",
        "        *   Minimum number of samples required to split a node.\n",
        "        *   Minimum number of samples required in a leaf node.\n",
        "        *   A threshold for the impurity measure (e.g., stop if information gain is below a certain value).\n",
        "    *   **Practical Advantage:** Pre-pruning is generally faster to implement and train because the tree is never allowed to grow to its full potential. This can save computational resources and time, especially with large datasets.\n",
        "\n",
        "*   **Post-pruning (Pruning after Growth):**\n",
        "    *   This technique involves growing the full Decision Tree first, potentially overfitting the training data.\n",
        "    *   After the tree is fully grown, nodes are removed or collapsed based on some evaluation metric, often using a validation set.\n",
        "    *   The goal is to remove branches that provide little or no improvement in accuracy on unseen data.\n",
        "    *   Techniques include cost-complexity pruning (weakest link pruning).\n",
        "    *   **Practical Advantage:** Post-pruning can sometimes lead to a more optimal tree than pre-pruning because it considers the full structure of the tree before making pruning decisions. It can potentially find better subtrees that might have been missed by stopping early.\n",
        "\n",
        "In summary, pre-pruning stops the tree from growing too large in the first place, while post-pruning trims back a fully grown tree. The choice between the two often depends on the specific problem, dataset size, and computational constraints."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques.4 What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n"
      ],
      "metadata": {
        "id": "k3hrpbOPUGea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:4"
      ],
      "metadata": {
        "id": "G7U_fvi_UQpM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8060a6fa"
      },
      "source": [
        "Ans: Information Gain is a key concept in Decision Trees used to determine the effectiveness of splitting a node based on a particular feature. It quantifies the reduction in entropy or Gini impurity achieved by a split.\n",
        "\n",
        "Here's why it's important for choosing the best split:\n",
        "\n",
        "1.  **Measures Impurity Reduction:** Information Gain directly measures how much a split helps in creating more homogeneous child nodes with respect to the class labels. A higher information gain means the split is more effective in separating the data into distinct classes.\n",
        "\n",
        "2.  **Guides Feature Selection:** At each node, the Decision Tree algorithm calculates the information gain for splitting on every available feature. The feature that yields the highest information gain is chosen as the splitting criterion for that node. This ensures that the tree prioritizes features that are most informative for classification.\n",
        "\n",
        "3.  **Leads to Efficient Trees:** By selecting splits that maximize information gain, the Decision Tree algorithm builds a tree that is typically more efficient and accurate in classifying new data points. It helps in creating a tree with fewer nodes and branches, making it easier to interpret and less prone to overfitting.\n",
        "\n",
        "In essence, Information Gain is the metric that guides the Decision Tree algorithm in making the best decisions at each node to build an effective classifier. It ensures that the tree is built by making splits that provide the most \"information\" about the class labels, leading to better predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques.5 What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "\n",
        "Dataset Info:\n",
        "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "● Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV)."
      ],
      "metadata": {
        "id": "DkOxWEhuUmoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:5"
      ],
      "metadata": {
        "id": "En21azCYUpwD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edabad8f"
      },
      "source": [
        "Ans: Decision Trees are versatile and widely used in various real-world applications due to their interpretability and ability to handle different types of data.\n",
        "\n",
        "**Common Real-World Applications:**\n",
        "\n",
        "*   **Medical Diagnosis:** Decision trees can be used to build models that help diagnose diseases based on patient symptoms and medical history.\n",
        "*   **Credit Risk Assessment:** Banks and financial institutions use decision trees to assess the creditworthiness of loan applicants based on their financial information.\n",
        "*   **Customer Relationship Management (CRM):** Decision trees can help businesses predict customer behavior, identify potential churn risks, and personalize marketing campaigns.\n",
        "*   **Fraud Detection:** Decision trees are used to detect fraudulent transactions in various domains, such as credit card fraud or insurance fraud.\n",
        "*   **Spam Filtering:** Email providers use decision trees to classify emails as spam or not spam based on the content and characteristics of the email.\n",
        "*   **Bioinformatics:** Decision trees can be applied to analyze biological data, such as gene expression data, to identify patterns and make predictions.\n",
        "*   **Manufacturing and Quality Control:** Decision trees can be used to identify factors that contribute to defects in manufacturing processes and improve quality control.\n",
        "\n",
        "**Main Advantages:**\n",
        "\n",
        "*   **Easy to Understand and Interpret:** Decision trees are visually intuitive and easy to understand, making them a good choice for explaining model predictions to non-experts.\n",
        "*   **Handles Both Numerical and Categorical Data:** Decision trees can work with both numerical and categorical features without requiring extensive data preprocessing like feature scaling.\n",
        "*   **Requires Little Data Preparation:** Compared to some other algorithms, decision trees require less data cleaning and preprocessing.\n",
        "*   **Non-linear Relationships:** Decision trees can capture non-linear relationships between features and the target variable.\n",
        "*   **Feature Selection:** The tree building process implicitly performs feature selection by prioritizing features that are most informative for splitting the data.\n",
        "\n",
        "**Main Limitations:**\n",
        "\n",
        "*   **Prone to Overfitting:** Decision trees can easily overfit the training data, especially if the tree is allowed to grow too deep. This can lead to poor performance on unseen data. Pruning techniques are used to mitigate this.\n",
        "*   **Instability:** Small changes in the training data can lead to significant changes in the structure of the decision tree. This can make the model less stable.\n",
        "*   **Bias towards Features with More Levels:** Decision trees can be biased towards features with a larger number of levels or categories, as they can create more splits.\n",
        "*   **Difficulty with Linearly Separable Data:** While decision trees handle non-linear relationships well, they might not be the best choice for datasets that are linearly separable, where simpler models like logistic regression might perform better.\n",
        "*   **Computationally Expensive (for large trees):** Growing a very large decision tree can be computationally expensive.\n",
        "\n",
        "Despite their limitations, Decision Trees remain a valuable tool in machine learning due to their interpretability and versatility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a475c0c1"
      },
      "source": [
        "# Task\n",
        "Load the Iris and Boston Housing datasets, train Decision Tree models for classification and regression respectively, and evaluate their performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4464dfa"
      },
      "source": [
        "## Load datasets\n",
        "\n",
        "### Subtask:\n",
        "Load the Iris dataset for classification and the Boston Housing dataset for regression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b895101c"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires loading the Iris and Boston Housing datasets. I will use `load_iris` and `load_boston` from `sklearn.datasets` to accomplish this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0967178d",
        "outputId": "6d3dbc68-bbef-4416-e414-3cd34d7a8d9a"
      },
      "source": [
        "from sklearn.datasets import load_iris, load_boston\n",
        "\n",
        "iris = load_iris()\n",
        "boston = load_boston()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3307078420.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_iris\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_boston\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0miris\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_iris\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mboston\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_boston\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/datasets/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \"\"\"\n\u001b[1;32m    160\u001b[0m         )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9b30660"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `load_boston` has been removed from scikit-learn. I need to use an alternative dataset for regression. The traceback suggests using `fetch_california_housing`. I will use `fetch_california_housing` to load the California Housing dataset and keep `load_iris` for the classification dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhtoQrGbVN_o"
      },
      "source": [
        "from sklearn.datasets import load_iris, fetch_california_housing\n",
        "\n",
        "iris = load_iris()\n",
        "boston = fetch_california_housing()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69f6dec6"
      },
      "source": [
        "## Explore datasets\n",
        "\n",
        "### Subtask:\n",
        "Briefly explore the datasets to understand their structure and content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79c592b7"
      },
      "source": [
        "**Reasoning**:\n",
        "Explore the structure and content of the loaded datasets as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3f5b3ab",
        "outputId": "083bd223-ff0b-4f7a-b012-bf1fdc76407a"
      },
      "source": [
        "# Explore Iris dataset\n",
        "print(\"Iris dataset keys:\", iris.keys())\n",
        "print(\"Iris data shape:\", iris.data.shape)\n",
        "print(\"Iris target shape:\", iris.target.shape)\n",
        "print(\"Iris feature names:\", iris.feature_names)\n",
        "print(\"Iris target names:\", iris.target_names)\n",
        "print(\"Iris description:\", iris.DESCR[:200] + \"...\") # Print only the first 200 characters for brevity\n",
        "\n",
        "# Explore Boston (California Housing) dataset\n",
        "print(\"\\nCalifornia Housing dataset keys:\", boston.keys())\n",
        "print(\"California Housing data shape:\", boston.data.shape)\n",
        "print(\"California Housing target shape:\", boston.target.shape)\n",
        "print(\"California Housing feature names:\", boston.feature_names)\n",
        "print(\"California Housing description:\", boston.DESCR[:200] + \"...\") # Print only the first 200 characters for brevity"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris dataset keys: dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n",
            "Iris data shape: (150, 4)\n",
            "Iris target shape: (150,)\n",
            "Iris feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "Iris target names: ['setosa' 'versicolor' 'virginica']\n",
            "Iris description: .. _iris_dataset:\n",
            "\n",
            "Iris plants dataset\n",
            "--------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            ":Number of Instances: 150 (50 in each of three classes)\n",
            ":Number of Attributes: 4 numeric, predictive attribu...\n",
            "\n",
            "California Housing dataset keys: dict_keys(['data', 'target', 'frame', 'target_names', 'feature_names', 'DESCR'])\n",
            "California Housing data shape: (20640, 8)\n",
            "California Housing target shape: (20640,)\n",
            "California Housing feature names: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
            "California Housing description: .. _california_housing_dataset:\n",
            "\n",
            "California Housing dataset\n",
            "--------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            ":Number of Instances: 20640\n",
            "\n",
            ":Number of Attributes: 8 numeric, predictive attribu...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e8c0601"
      },
      "source": [
        "## Implement decision tree classifier\n",
        "\n",
        "### Subtask:\n",
        "Train a Decision Tree model on the Iris dataset for classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64817133"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary modules and split the Iris dataset into training and testing sets, then instantiate and train a Decision Tree Classifier.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "9646bef2",
        "outputId": "dc57ae02-e0fb-4b0f-d544-ff3e57cfd37a"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>DecisionTreeClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(random_state=42)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "196d341e"
      },
      "source": [
        "## Implement decision tree regressor\n",
        "\n",
        "### Subtask:\n",
        "Train a Decision Tree model on the Boston Housing dataset for regression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fbc29ca"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary class, split the data, instantiate the model, and train it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "eb9172ca",
        "outputId": "faf37ae6-78ec-4a37-e8d5-e387a53532bc"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(boston.data, boston.target, test_size=0.2, random_state=42)\n",
        "\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "dt_regressor.fit(X_train_reg, y_train_reg)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-2 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-2 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-2 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-2 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-2 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>DecisionTreeRegressor</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.tree.DecisionTreeRegressor.html\">?<span>Documentation for DecisionTreeRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeRegressor(random_state=42)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ec83598"
      },
      "source": [
        "## Evaluate models\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of both the classifier and regressor models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65455efb"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary evaluation metrics for both classification and regression models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30505971"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, r2_score"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "175246ab"
      },
      "source": [
        "**Reasoning**:\n",
        "Predict the target values for the test set using the trained classification model and calculate the classification evaluation metrics. Then, predict the target values for the test set using the trained regression model and calculate the regression evaluation metrics. Finally, print the calculated evaluation metrics for both models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9ea5b7d",
        "outputId": "a7c8e3ae-9689-4179-e497-ee28f8bf4d4b"
      },
      "source": [
        "y_pred_classifier = dt_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred_classifier)\n",
        "precision = precision_score(y_test, y_pred_classifier, average='weighted')\n",
        "recall = recall_score(y_test, y_pred_classifier, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred_classifier, average='weighted')\n",
        "\n",
        "print(\"Classification Model Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "y_pred_regressor = dt_regressor.predict(X_test_reg)\n",
        "mse = mean_squared_error(y_test_reg, y_pred_regressor)\n",
        "r2 = r2_score(y_test_reg, y_pred_regressor)\n",
        "\n",
        "print(\"\\nRegression Model Evaluation:\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"R-squared (R2) Score: {r2:.4f}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Model Evaluation:\n",
            "Accuracy: 1.0000\n",
            "Precision: 1.0000\n",
            "Recall: 1.0000\n",
            "F1-score: 1.0000\n",
            "\n",
            "Regression Model Evaluation:\n",
            "Mean Squared Error (MSE): 0.4952\n",
            "R-squared (R2) Score: 0.6221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3930609"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The Iris dataset, used for classification, contains 150 samples with 4 features (`sepal length (cm)`, `sepal width (cm)`, `petal length (cm)`, `petal width (cm)`) and 3 target classes (`setosa`, `versicolor`, `virginica`).\n",
        "*   The Boston Housing dataset (`load_boston`) is no longer available in scikit-learn due to ethical concerns. The California Housing dataset (`fetch_california_housing`) was used as a suitable replacement for regression, containing 20640 samples with 8 features and a target variable representing median house values.\n",
        "*   The Decision Tree classifier achieved perfect performance on the Iris test set, with an accuracy of 1.0000, precision of 1.0000, recall of 1.0000, and an F1-score of 1.0000.\n",
        "*   The Decision Tree regressor on the California Housing dataset resulted in a Mean Squared Error (MSE) of 0.4952 and an R-squared (\\(R^2\\)) score of 0.6221 on the test set.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The perfect scores of the Decision Tree classifier on the Iris dataset suggest potential overfitting. Further evaluation with cross-validation or exploring model complexity tuning could provide a more robust assessment.\n",
        "*   The regression model's performance, while reasonable, could be improved by exploring different regression algorithms, feature engineering, or hyperparameter tuning for the Decision Tree Regressor.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques.6 Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances?"
      ],
      "metadata": {
        "id": "1pYciH-QVkkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:6"
      ],
      "metadata": {
        "id": "ul7HqnJiVqG8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f2b87bd",
        "outputId": "a4a35224-2bda-47bc-a7c7-5e7dea01d8ef"
      },
      "source": [
        "# Calculate accuracy on the test set\n",
        "accuracy = dt_classifier.score(X_test, y_test)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = dt_classifier.feature_importances_\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(iris.feature_names, feature_importances):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0000\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques.7 Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree?"
      ],
      "metadata": {
        "id": "aaAGbheVVwS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:7"
      ],
      "metadata": {
        "id": "Xt463PvWV0em"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d50cb4f",
        "outputId": "f7ba56bf-74dc-4b4e-821f-f24aa40094c5"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset (if not already loaded)\n",
        "# from sklearn.datasets import load_iris\n",
        "# iris = load_iris()\n",
        "\n",
        "# Split data into training and testing sets (if not already split)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier with max_depth=3\n",
        "dt_classifier_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "dt_classifier_limited.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate the limited depth tree\n",
        "y_pred_limited = dt_classifier_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Get accuracy of the fully-grown tree (assuming dt_classifier is the fully-grown tree)\n",
        "accuracy_full = dt_classifier.score(X_test, y_test)\n",
        "\n",
        "print(f\"Accuracy of Decision Tree with max_depth=3: {accuracy_limited:.4f}\")\n",
        "print(f\"Accuracy of fully-grown Decision Tree: {accuracy_full:.4f}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree with max_depth=3: 1.0000\n",
            "Accuracy of fully-grown Decision Tree: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques.8 : Write a Python program to:\n",
        "● Load the Boston Housing Dataset\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances?"
      ],
      "metadata": {
        "id": "p_F0d7pGWhOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:8"
      ],
      "metadata": {
        "id": "lokjQe3IWmKl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4be85f2",
        "outputId": "d56627f3-9c61-401a-b3b7-711aae9d2fd8"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import pandas as pd\n",
        "\n",
        "# Load the California Housing dataset\n",
        "boston = fetch_california_housing()\n",
        "X_reg = boston.data\n",
        "y_reg = boston.target\n",
        "feature_names_reg = boston.feature_names\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "dt_regressor.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_regressor = dt_regressor.predict(X_test_reg)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test_reg, y_pred_regressor)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances_reg = dt_regressor.feature_importances_\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(feature_names_reg, feature_importances_reg):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.4952\n",
            "Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques.9 Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy?"
      ],
      "metadata": {
        "id": "jgZIo48jWzBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:9"
      ],
      "metadata": {
        "id": "tDT_2ca-W_gO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71421d48",
        "outputId": "e1f23a94-1eaf-42a9-b383-099db554dcd9"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset (if not already loaded)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets (if not already split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [None, 2, 3, 4, 5],\n",
        "    'min_samples_split': [2, 5, 10, 15, 20]\n",
        "}\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(dt_classifier, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_dt_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on the test set using the best model\n",
        "y_pred_tuned = best_dt_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the best model\n",
        "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
        "print(f\"\\nAccuracy of the best model on the test set: {accuracy_tuned:.4f}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found by GridSearchCV:\n",
            "{'max_depth': None, 'min_samples_split': 2}\n",
            "\n",
            "Accuracy of the best model on the test set: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques.10 Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting?"
      ],
      "metadata": {
        "id": "82rtTKL_XKXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:10"
      ],
      "metadata": {
        "id": "fni0k8luXWCQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56c9e83b"
      },
      "source": [
        "Ans: Here is a step-by-step process for building a Decision Tree model to predict whether a patient has a certain disease, considering a dataset with mixed data types and missing values:\n",
        "\n",
        "**Step-by-Step Process:**\n",
        "\n",
        "1.  **Understand the Data:**\n",
        "    *   Thoroughly examine the dataset to understand the features, their data types (numerical, categorical), and the target variable (disease present/absent).\n",
        "    *   Identify the presence and nature of missing values and outliers.\n",
        "\n",
        "2.  **Handle Missing Values:**\n",
        "    *   **Identify Missing Data:** Determine which features have missing values and the extent of missingness.\n",
        "    *   **Choose Imputation Strategy:** Select appropriate methods for handling missing data based on the feature type and the nature of missingness.\n",
        "        *   For numerical features: Impute with the mean, median, or mode. More advanced techniques like K-Nearest Neighbors (KNN) imputation or multiple imputation could also be considered.\n",
        "        *   For categorical features: Impute with the mode or a placeholder category like \"Unknown\".\n",
        "    *   **Implement Imputation:** Apply the chosen imputation strategies to fill in the missing values.\n",
        "\n",
        "3.  **Encode Categorical Features:**\n",
        "    *   **Identify Categorical Features:** Determine which features are categorical.\n",
        "    *   **Choose Encoding Method:** Select appropriate encoding methods.\n",
        "        *   **One-Hot Encoding:** For nominal categorical features where there is no inherent order (e.g., blood type). This creates new binary columns for each category.\n",
        "        *   **Ordinal Encoding:** For ordinal categorical features where there is a meaningful order (e.g., disease severity: mild, moderate, severe). This assigns numerical labels based on the order.\n",
        "    *   **Implement Encoding:** Apply the chosen encoding methods to convert categorical features into a numerical format that the Decision Tree can process.\n",
        "\n",
        "4.  **Split the Data:**\n",
        "    *   Divide the dataset into training, validation (optional but recommended for hyperparameter tuning), and testing sets. A common split is 70-80% for training, and the remaining for testing (with a validation set split from the training data). This ensures that the model is evaluated on unseen data.\n",
        "\n",
        "5.  **Train a Decision Tree Model:**\n",
        "    *   Import the `DecisionTreeClassifier` from `sklearn.tree`.\n",
        "    *   Instantiate the classifier.\n",
        "    *   Train the model using the training data (`fit(X_train, y_train)`). Start with a basic Decision Tree without specific hyperparameter tuning to get a baseline performance.\n",
        "\n",
        "6.  **Tune Hyperparameters:**\n",
        "    *   **Identify Important Hyperparameters:** For Decision Trees, key hyperparameters to tune include `max_depth` (maximum depth of the tree), `min_samples_split` (minimum number of samples required to split an internal node), `min_samples_leaf` (minimum number of samples required to be at a leaf node), and `criterion` (Gini or Entropy).\n",
        "    *   **Define a Parameter Grid:** Create a dictionary or list of dictionaries specifying the hyperparameters and the range of values to explore.\n",
        "    *   **Use a Tuning Method:** Employ techniques like GridSearchCV or RandomizedSearchCV from `sklearn.model_selection` to systematically search for the best combination of hyperparameters based on a chosen evaluation metric (e.g., accuracy, precision, recall, F1-score). Use cross-validation during tuning to get a more reliable estimate of performance.\n",
        "    *   **Train with Best Parameters:** Train the final Decision Tree model on the training data using the best hyperparameters found during tuning.\n",
        "\n",
        "7.  **Evaluate Performance:**\n",
        "    *   **Predict on the Test Set:** Use the trained model with the best hyperparameters to make predictions on the unseen test set (`predict(X_test)`).\n",
        "    *   **Calculate Evaluation Metrics:** Assess the model's performance using appropriate classification metrics. For disease prediction, metrics like:\n",
        "        *   **Accuracy:** Overall correct predictions.\n",
        "        *   **Precision:** Of those predicted positive, how many were actually positive (minimizing false positives is crucial in healthcare).\n",
        "        *   **Recall (Sensitivity):** Of those actually positive, how many were correctly predicted positive (minimizing false negatives is crucial).\n",
        "        *   **F1-score:** Harmonic mean of precision and recall.\n",
        "        *   **AUC-ROC Curve:** Measures the model's ability to distinguish between positive and negative classes.\n",
        "    *   **Interpret Results:** Analyze the evaluation metrics to understand the model's strengths and weaknesses. Consider the business context and which metrics are most important (e.g., is it more critical to minimize false positives or false negatives?).\n",
        "\n",
        "**Business Value in Real-World Setting:**\n",
        "\n",
        "A Decision Tree model for predicting a certain disease could provide significant business value in a healthcare setting:\n",
        "\n",
        "*   **Early Detection and Intervention:** Identifying patients at high risk of developing a disease early allows for timely intervention, potentially leading to better patient outcomes and reduced healthcare costs.\n",
        "*   **Resource Allocation:** The model can help healthcare providers prioritize resources by focusing on patients who are most likely to need care.\n",
        "*   **Personalized Treatment:** Understanding the factors that contribute to disease risk (from feature importances) can help in developing personalized treatment plans.\n",
        "*   **Improved Patient Management:** The model can assist in managing patient populations by identifying those who require closer monitoring or preventative measures.\n",
        "*   **Reduced Healthcare Costs:** Early detection and intervention can prevent the progression of diseases, reducing the need for more expensive treatments later on.\n",
        "*   **Enhanced Research:** The tree structure can provide insights into the relationships between different patient characteristics and the likelihood of disease, which can inform further medical research.\n",
        "*   **Supporting Clinical Decision-Making:** While not replacing clinical judgment, the model can serve as a valuable tool to support doctors in making more informed decisions.\n",
        "\n",
        "By following this process, a healthcare company can leverage machine learning to improve patient care, optimize resource utilization, and ultimately contribute to better health outcomes."
      ]
    }
  ]
}